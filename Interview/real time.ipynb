{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bfb8ae-1da0-4367-ac60-c4927bf83b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "Removing Excess Buckets from Cluster Master\n",
    "Excess bucket copies are copies that exceed the cluster's replication factor (RF) or search factor (SF).\n",
    "For example, if the RF is 3, each bucket should have exactly 3 copies across peer nodes.\n",
    "If one bucket has 4 copies, then it has 1 excess copy.\n",
    "\n",
    "Note: Excess copies do not affect cluster operations, but they consume extra disk space.\n",
    "\n",
    "Method 1: From GUI\n",
    "Ensure all peers are communicating with the Cluster Master (CM).\n",
    "Restart the splunkd service on the Cluster Master.\n",
    "Remove the excess bucket from the Cluster Master.\n",
    "Sometimes the process gets stuck on the CM. Restarting splunkd usually resolves the issue.\n",
    "\n",
    "Method 2: From CLI\n",
    "Ensure all peers are communicating with the Cluster Master.\n",
    "\n",
    "Restart the splunkd service on the Cluster Master.\n",
    "\n",
    "Run the following commands:\n",
    "\n",
    "# List excess buckets for a specific index\n",
    "splunk list excess-buckets <index-name>\n",
    "\n",
    "# Remove excess buckets for a specific index\n",
    "splunk remove excess-buckets <index-name>\n",
    "Search Artifacts Dispatch Error\n",
    "You may encounter the following warning:\n",
    "\n",
    "The number of search artifacts in the dispatch directory is higher than recommended\n",
    "(count=5360, warning threshold=5000). This may impact search performance.\n",
    "\n",
    "Root Cause\n",
    "The warning is related to the dispatch_dir_warning_size attribute in limits.conf.\n",
    "It occurs when the number of search artifacts in the dispatch directory exceeds the recommended threshold.\n",
    "Default threshold: 5000.\n",
    "Attribute Details\n",
    "dispatch\\_dir\\_warning\\_size = <int>\n",
    "* Specifies the number of jobs in the dispatch directory that triggers a warning.\n",
    "* Default: 5000\n",
    "Observations\n",
    "The dispatch directory is located at:\n",
    "$SPLUNK_HOME/var/run/splunk/dispatch\n",
    "This directory stores search artifacts.\n",
    "\n",
    "A large number of searches can accumulate here, exceeding the threshold.\n",
    "When exceeded, Splunk issues a performance warning.\n",
    "Resolution Steps\n",
    "Locate the limits.conf file\n",
    "vi /opt/splunk/etc/system/local/limits.conf\n",
    "If you’re unsure of the exact location, run:\n",
    "\n",
    "splunk btool limits list --debug | grep dispatch_dir_warning_size\n",
    "Modify the parameter Increase the threshold (default is 5000). Example:\n",
    "\n",
    "[search]\n",
    "dispatch_dir_warning_size = 10000\n",
    "Restart Splunk\n",
    "\n",
    "$SPLUNK_HOME/bin/splunk restart\n",
    "After restarting, Splunk will use the updated threshold for search artifact warnings.\n",
    "\n",
    "Logfile Data Not Coming in Correct Form\n",
    "Issue\n",
    "The data in the logfile is in Japanese language, but after indexing in Splunk, it appears incorrectly (garbled characters).\n",
    "Root Cause\n",
    "Splunk may not be detecting the correct character set encoding during data ingestion.\n",
    "For Japanese logs, the supported encodings are:\n",
    "\n",
    "EUC-JP\n",
    "SHIFT-JIS\n",
    "Reference: Splunk Documentation - Configure character set encoding\n",
    "\n",
    "Resolution\n",
    "You need to manually specify the character set in props.conf.\n",
    "\n",
    "Steps:\n",
    "Open or create props.conf in your configuration directory:\n",
    "   vi $SPLUNK_HOME/etc/system/local/props.conf\n",
    "Add a new stanza for the sourcetype of your input. Example:\n",
    "\n",
    "[mysourcetype]\n",
    "CHARSET = EUC-JP\n",
    "Or, if EUC-JP does not work, try:\n",
    "\n",
    "[mysourcetype]\n",
    "CHARSET = SHIFT-JIS\n",
    "Save the file and restart Splunk:\n",
    "\n",
    "$SPLUNK_HOME/bin/splunk restart\n",
    "Notes\n",
    "You may need to test both encodings (EUC-JP and SHIFT-JIS) to identify which one correctly parses your log data.\n",
    "Always configure this on the indexing tier (indexers or heavy forwarders), not just on universal forwarders.\n",
    "Here’s your content formatted in Markdown for clear documentation:\n",
    "\n",
    "KV Store Changed Status to Failed (Process Terminated)\n",
    "Error Details\n",
    "splunkd.log\n",
    "11-04-2020 10:26:13.265 +1100 ERROR MongodRunner - mongod exited abnormally (exit code 14, status: exited with code 14) - look at mongod.log to investigate.\n",
    "11-04-2020 10:26:13.269 +1100 ERROR KVStoreBulletinBoardManager - KV Store process terminated abnormally (exit code 14, status exited with code 14). See mongod.log and splunkd.log for details.\n",
    "11-04-2020 10:26:13.269 +1100 ERROR KVStoreBulletinBoardManager - KV Store changed status to failed. KVStore process terminated..\n",
    "mongod.log\n",
    "2020-10-26T00:34:50.369Z I CONTROL \\[initandlisten] \\*\\* WARNING: No SSL certificate validation can be performed since no CA file has been provided\n",
    "2020-10-19T03:56:45.683Z I CONTROL \\[initandlisten] \\*\\* Please specify an sslCAFile parameter.\n",
    "2020-11-03T23:26:13.263Z F - \\[main] Fatal Assertion 28652\n",
    "\\*\\*\\*aborting after fassert() failure\n",
    "Root Cause\n",
    "The KV Store (MongoDB process) failed due to issues with the SSL certificate (server.pem). If the SSL certificate has expired or is invalid, MongoDB cannot start, causing KV Store failure.\n",
    "\n",
    "Procedure to Renew SSL Certificate\n",
    "Check if the SSL certificate is expired\n",
    "\n",
    "openssl x509 -enddate -noout -in $SPLUNK_HOME/etc/auth/server.pem\n",
    "Backup the existing certificate\n",
    "\n",
    "cp $SPLUNK_HOME/etc/auth/server.pem $SPLUNK_HOME/etc/auth/server.pem.bak\n",
    "Generate a new SSL certificate\n",
    "\n",
    "$SPLUNK_HOME/bin/splunk createssl server-cert \\\n",
    "-d $SPLUNK_HOME/etc/auth \\\n",
    "-n server \\\n",
    "-c SplunkServerDefaultCert \\\n",
    "-l 2048\n",
    "Restart Splunk\n",
    "\n",
    "$SPLUNK_HOME/bin/splunk restart\n",
    "Verify KV Store status\n",
    "\n",
    "$SPLUNK_HOME/bin/splunk show kvstore-status\n",
    "Alternate Quick Fix\n",
    "Rename or delete the existing server.pem file:\n",
    "\n",
    "mv $SPLUNK_HOME/etc/auth/server.pem $SPLUNK_HOME/etc/auth/server.pem.old\n",
    "Restart Splunk. Splunk will automatically generate a new server.pem on startup.\n",
    "\n",
    "After this procedure, the KV Store should start successfully.\n",
    "\n",
    "Indexer Down in Cluster (One Indexer Out of 6)\n",
    "Issue\n",
    "Out of 6 indexers in the cluster, one indexer shows as Down.\n",
    "\n",
    "splunkd.log\n",
    "12-09-2020 12:56:51.331 +0630 WARN CMSlave - Failed to register with cluster master reason: failed method=POST path=/services/cluster/master/peers/?output\\_mode=json master=10.75.49.21:8089 rv=0 gotConnectionError=0 gotUnexpectedStatusCode=1 actual\\_response\\_code=500 expected\\_response\\_code=2xx status\\_line=\"Internal Server Error\" socket\\_error=\"No error\" remote\\_error=Cannot add peer=10.75.49.25 mgmtport=8089 (reason: bucket already added as clustered, peer attempted to add again as standalone. guid=347A3164-2E3B-4D9D-8603-A17180A9E92E bid=tml\\_app\\_paloalto~~170~~347A3164-2E3B-4D9D-8603-A17180A9E92E).\n",
    "Root Cause\n",
    "The indexer attempted to register with the Cluster Master (CM) but failed because some buckets were detected as standalone buckets instead of clustered ones.\n",
    "\n",
    "These standalone buckets are missing the cluster GUID in their name.\n",
    "This mismatch prevents the peer from rejoining the cluster.\n",
    "Resolution Steps\n",
    "Search for standalone buckets\n",
    "Navigate to the indexer data path:\n",
    "cd /opt/splunk/var/lib/splunk/\n",
    "Use the command to find problematic buckets:\n",
    "\n",
    "find /opt/splunk -name \"db_*\"\n",
    "Standalone bucket naming convention:\n",
    "\n",
    "db_<newest_time>_<oldest_time>_<bucketid>\n",
    "Example:\n",
    "\n",
    "db_1550812574_1550720467_53\n",
    "Append the Cluster Master GUID Rename each standalone bucket by appending the cluster master GUID to the end.\n",
    "\n",
    "From:\n",
    "\n",
    "db_<newest_time>_<oldest_time>_<bucketid>\n",
    "To:\n",
    "\n",
    "db_<newest_time>_<oldest_time>_<bucketid>_<guid>\n",
    "Example:\n",
    "\n",
    "mv db_1550812574_1550720467_53 \\\n",
    "   db_1550812574_1550720467_53_347A3164-2E3B-4D9D-8603-A17180A9E92E\n",
    "(Here, guid=347A3164-2E3B-4D9D-8603-A17180A9E92E)\n",
    "\n",
    "Reference: Splunk Docs - Bucket names\n",
    "\n",
    "Restart the Indexer\n",
    "\n",
    "$SPLUNK_HOME/bin/splunk restart\n",
    "The search peer should now rejoin the cluster.\n",
    "\n",
    "Disable Maintenance Mode (if enabled on CM).\n",
    "\n",
    "Allow Bucket Fix-up Tasks Wait for the cluster to complete fix-up processing.\n",
    "\n",
    "Important Notes\n",
    "Ensure pass4symmkey is the same across all indexers and the Cluster Master.\n",
    "\n",
    "Verify that the instance GUID is present in:\n",
    "\n",
    "$SPLUNK_HOME/etc/instance.cfg\n",
    "After these steps, the indexer should rejoin the cluster successfully."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
